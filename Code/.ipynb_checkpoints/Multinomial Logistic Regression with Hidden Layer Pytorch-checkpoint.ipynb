{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from evaluate import *\n",
    "from load_test_set_pytorch import *\n",
    "from load_dataset_rgb_scale import *\n",
    "from __future__ import print_function\n",
    "from torch.autograd import Variable\n",
    "\n",
    "numClass = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 64, 64, 3)\n",
      "(39209,)\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_dataset_rgb_scale()\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_dimension(x):\n",
    "    x = np.transpose(x, (0, 3, 1, 2))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "X = transpose_dimension(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, padding=1) #using \"same\" with relu then max pool from 64x64x3 into 32x32x6\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3, padding=1) #using \"same\" with relu then max pool from 32x32x6 to 16x16x8\n",
    "        self.conv3 = nn.Conv2d(12, 16, 3, padding=1) #using \"same\" with relu then max pool from 16x16x8 to 8x8x16\n",
    "        self.conv4 = nn.Conv2d(16, 25, 3, padding=1) #using \"same\" with relu then max pool from 8x8x16 to 4x4x32\n",
    "        #Fully connected layer\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 43)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.max_pool2d(F.relu(self.conv1(X)), (2,2))\n",
    "        X = F.max_pool2d(F.relu(self.conv2(X)), (2,2))\n",
    "        X = F.max_pool2d(F.relu(self.conv3(X)), (2,2))\n",
    "        X = F.max_pool2d(F.relu(self.conv4(X)), (2,2))\n",
    "        X = X.view(-1, self.num_flat_features(X))\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = F.relu(self.fc3(X))\n",
    "        X = self.fc4(X)\n",
    "        return X\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(checkpoint_path, store_type,\n",
    "               learning_rate = (1e-4)*5, num_epochs = 5, batch_size = 512, resume = False, num_class = 43):\n",
    "    net = Net()\n",
    "    start = time.time()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    iters_per_epoch = int(m / batch_size)+1\n",
    "    hist = []\n",
    "    \n",
    "    name_checkpoint = \"model_epoch\" + store_type + \".chkpt\"\n",
    "    \n",
    "    if (resume):\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_path, name_checkpoint))\n",
    "        hist = checkpoint[\"hist\"]\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        net.load_state_dict(checkpoint[\"net\"])\n",
    "    \n",
    "    cnt = 0\n",
    "    for param_group in optimizer.param_groups: \n",
    "        param_group['lr'] = learning_rate\n",
    "        cnt += 1\n",
    "    print(cnt)\n",
    "    \n",
    "    for iters in range(num_epochs):\n",
    "        for i in range(iters_per_epoch):\n",
    "            indices = np.random.choice(m, batch_size)\n",
    "            \n",
    "            input = X[indices]\n",
    "            target = Y[indices]\n",
    "            \n",
    "            input = Variable(torch.from_numpy(input).float())\n",
    "            target = Variable(torch.from_numpy(target).long())\n",
    "            \n",
    "            out = net(input)\n",
    "            loss = criterion(out, target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            net.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i%20==0):\n",
    "                now = time.time()\n",
    "                print(\"iterate %d: loss = %.9f, spent = %.5f\" % (i + 1, loss.data[0], now - start))\n",
    "                start = now\n",
    "\n",
    "        print(\"Epoch %d: loss = %.9f\" % (iters + 1, loss.data[0]))\n",
    "        hist.append(loss.data[0])\n",
    "        \n",
    "    #\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    checkpoint = {\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"hist\": hist,\n",
    "        \"net\": net.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(checkpoint_path, name_checkpoint))\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "iterate 1: loss = 0.081880197, spent = 1.51758\n",
      "iterate 21: loss = 0.076709844, spent = 28.00225\n",
      "iterate 41: loss = 0.074760832, spent = 29.26493\n",
      "iterate 61: loss = 0.102932006, spent = 29.48433\n",
      "Epoch 1: loss = 0.084919825\n",
      "iterate 1: loss = 0.087615699, spent = 25.08452\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-acde672ab0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"7\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-b68f4f9cd569>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(checkpoint_path, store_type, learning_rate, num_epochs, batch_size, resume, num_class)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = train_data(\"checkpoint\", \"7\", resume = True, num_epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.346059560775757, 2.1516213417053223, 1.638705849647522, 1.3524199724197388, 0.9780049324035645, 0.6142547130584717, 0.5039753913879395, 0.3777349889278412, 0.21239349246025085, 0.1960049420595169, 0.22749817371368408, 0.11804602295160294, 0.10003401339054108, 0.08535657078027725, 0.10445734858512878, 0.10273575782775879, 0.13434825837612152, 0.07329681515693665, 0.10131028294563293, 0.08707845956087112, 0.04248499497771263, 0.04817185550928116, 0.0683213621377945, 0.06822340190410614, 0.05894487351179123]\n",
      "Net(\n",
      "  (conv1): Conv2d (3, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d (8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d (16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d (32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=1024, out_features=512)\n",
      "  (fc2): Linear(in_features=512, out_features=256)\n",
      "  (fc3): Linear(in_features=256, out_features=128)\n",
      "  (fc4): Linear(in_features=128, out_features=43)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.join(\"checkpoint\", \"model_epoch7.chkpt\"))\n",
    "\n",
    "net = Net()\n",
    "\n",
    "hist = checkpoint[\"hist\"]\n",
    "net.load_state_dict(checkpoint[\"net\"])\n",
    "\n",
    "print(hist)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12630, 64, 64, 3)\n",
      "(12630,)\n"
     ]
    }
   ],
   "source": [
    "test_X, test_Y = load_test_set()\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12630, 3, 64, 64)\n",
      "(3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "test_X = transpose_dimension(test_X)\n",
    "print(test_X.shape)\n",
    "print(test_X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(net, instances):\n",
    "    instances = Variable(instances)\n",
    "    outputs = net(instances)\n",
    "    return outputs\n",
    "\n",
    "def evaluate(net, data_X, data_Y):\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    m = data_X.shape[0]\n",
    "\n",
    "    # Switch to evaluation mode.\n",
    "    net.eval()\n",
    "    \n",
    "    for i in range(m):\n",
    "        outputs = get_outputs(net, torch.from_numpy(data_X[i:i+1]).float())\n",
    "        labels = Variable(torch.from_numpy(data_Y[i:i+1]).long())\n",
    "\n",
    "        loss += nn.CrossEntropyLoss(size_average=False)(outputs, labels).data[0]\n",
    "\n",
    "        score, predicted = torch.max(outputs, 1)\n",
    "        now = (labels.data == predicted.data).sum()\n",
    "        correct += now\n",
    "        \n",
    "        if (now==0):\n",
    "            print(i)\n",
    "            \n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    loss /= total\n",
    "\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "13\n",
      "14\n",
      "33\n",
      "39\n",
      "40\n",
      "49\n",
      "50\n",
      "52\n",
      "68\n",
      "79\n",
      "83\n",
      "85\n",
      "102\n",
      "107\n",
      "114\n",
      "117\n",
      "118\n",
      "122\n",
      "127\n",
      "131\n",
      "143\n",
      "145\n",
      "147\n",
      "150\n",
      "151\n",
      "159\n",
      "163\n",
      "176\n",
      "190\n",
      "191\n",
      "197\n",
      "198\n",
      "204\n",
      "213\n",
      "229\n",
      "235\n",
      "237\n",
      "250\n",
      "265\n",
      "280\n",
      "287\n",
      "305\n",
      "306\n",
      "309\n",
      "310\n",
      "335\n",
      "339\n",
      "341\n",
      "349\n",
      "355\n",
      "358\n",
      "368\n",
      "374\n",
      "379\n",
      "380\n",
      "393\n",
      "421\n",
      "423\n",
      "427\n",
      "446\n",
      "455\n",
      "459\n",
      "461\n",
      "475\n",
      "486\n",
      "490\n",
      "492\n",
      "496\n",
      "499\n",
      "502\n",
      "503\n",
      "537\n",
      "540\n",
      "542\n",
      "550\n",
      "567\n",
      "586\n",
      "600\n",
      "603\n",
      "606\n",
      "613\n",
      "622\n",
      "624\n",
      "628\n",
      "660\n",
      "677\n",
      "682\n",
      "715\n",
      "745\n",
      "748\n",
      "749\n",
      "755\n",
      "758\n",
      "760\n",
      "764\n",
      "781\n",
      "790\n",
      "804\n",
      "814\n",
      "815\n",
      "829\n",
      "860\n",
      "882\n",
      "885\n",
      "901\n",
      "914\n",
      "917\n",
      "919\n",
      "924\n",
      "937\n",
      "939\n",
      "943\n",
      "966\n",
      "982\n",
      "992\n",
      "995\n",
      "1008\n",
      "1011\n",
      "1034\n",
      "1044\n",
      "1045\n",
      "1061\n",
      "1064\n",
      "1065\n",
      "1069\n",
      "1079\n",
      "1080\n",
      "1084\n",
      "1105\n",
      "1109\n",
      "1121\n",
      "1122\n",
      "1125\n",
      "1131\n",
      "1136\n",
      "1137\n",
      "1168\n",
      "1171\n",
      "1182\n",
      "1183\n",
      "1189\n",
      "1231\n",
      "1234\n",
      "1261\n",
      "1271\n",
      "1275\n",
      "1276\n",
      "1298\n",
      "1303\n",
      "1307\n",
      "1317\n",
      "1323\n",
      "1352\n",
      "1364\n",
      "1373\n",
      "1384\n",
      "1385\n",
      "1389\n",
      "1402\n",
      "1404\n",
      "1431\n",
      "1440\n",
      "1455\n",
      "1466\n",
      "1472\n",
      "1475\n",
      "1489\n",
      "1490\n",
      "1492\n",
      "1496\n",
      "1499\n",
      "1522\n",
      "1524\n",
      "1552\n",
      "1560\n",
      "1573\n",
      "1574\n",
      "1577\n",
      "1579\n",
      "1583\n",
      "1592\n",
      "1593\n",
      "1604\n",
      "1612\n",
      "1620\n",
      "1621\n",
      "1643\n",
      "1649\n",
      "1656\n",
      "1660\n",
      "1662\n",
      "1665\n",
      "1671\n",
      "1676\n",
      "1704\n",
      "1706\n",
      "1709\n",
      "1710\n",
      "1734\n",
      "1738\n",
      "1743\n",
      "1746\n",
      "1747\n",
      "1759\n",
      "1762\n",
      "1763\n",
      "1767\n",
      "1774\n",
      "1782\n",
      "1790\n",
      "1802\n",
      "1804\n",
      "1812\n",
      "1819\n",
      "1857\n",
      "1861\n",
      "1868\n",
      "1873\n",
      "1884\n",
      "1889\n",
      "1898\n",
      "1909\n",
      "1910\n",
      "1924\n",
      "1935\n",
      "1938\n",
      "1952\n",
      "1955\n",
      "1968\n",
      "1969\n",
      "1983\n",
      "1987\n",
      "2002\n",
      "2019\n",
      "2025\n",
      "2041\n",
      "2061\n",
      "2065\n",
      "2069\n",
      "2076\n",
      "2079\n",
      "2081\n",
      "2086\n",
      "2102\n",
      "2109\n",
      "2112\n",
      "2151\n",
      "2154\n",
      "2161\n",
      "2176\n",
      "2179\n",
      "2182\n",
      "2191\n",
      "2198\n",
      "2218\n",
      "2220\n",
      "2229\n",
      "2235\n",
      "2240\n",
      "2249\n",
      "2258\n",
      "2276\n",
      "2295\n",
      "2317\n",
      "2332\n",
      "2340\n",
      "2342\n",
      "2345\n",
      "2353\n",
      "2354\n",
      "2370\n",
      "2373\n",
      "2376\n",
      "2378\n",
      "2379\n",
      "2383\n",
      "2393\n",
      "2394\n",
      "2396\n",
      "2402\n",
      "2407\n",
      "2425\n",
      "2469\n",
      "2472\n",
      "2477\n",
      "2482\n",
      "2483\n",
      "2494\n",
      "2503\n",
      "2512\n",
      "2529\n",
      "2538\n",
      "2546\n",
      "2547\n",
      "2577\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2611\n",
      "2615\n",
      "2617\n",
      "2625\n",
      "2635\n",
      "2637\n",
      "2645\n",
      "2651\n",
      "2669\n",
      "2682\n",
      "2699\n",
      "2717\n",
      "2720\n",
      "2736\n",
      "2742\n",
      "2751\n",
      "2758\n",
      "2768\n",
      "2789\n",
      "2791\n",
      "2793\n",
      "2803\n",
      "2805\n",
      "2810\n",
      "2812\n",
      "2817\n",
      "2820\n",
      "2833\n",
      "2836\n",
      "2847\n",
      "2849\n",
      "2854\n",
      "2881\n",
      "2886\n",
      "2887\n",
      "2891\n",
      "2896\n",
      "2907\n",
      "2913\n",
      "2922\n",
      "2924\n",
      "2930\n",
      "2931\n",
      "2946\n",
      "2969\n",
      "2970\n",
      "2980\n",
      "2993\n",
      "2994\n",
      "2996\n",
      "3018\n",
      "3025\n",
      "3027\n",
      "3028\n",
      "3030\n",
      "3037\n",
      "3051\n",
      "3052\n",
      "3056\n",
      "3063\n",
      "3086\n",
      "3101\n",
      "3109\n",
      "3142\n",
      "3147\n",
      "3157\n",
      "3161\n",
      "3167\n",
      "3173\n",
      "3207\n",
      "3210\n",
      "3212\n",
      "3217\n",
      "3220\n",
      "3230\n",
      "3234\n",
      "3242\n",
      "3245\n",
      "3260\n",
      "3285\n",
      "3288\n",
      "3293\n",
      "3303\n",
      "3329\n",
      "3344\n",
      "3362\n",
      "3363\n",
      "3370\n",
      "3380\n",
      "3389\n",
      "3402\n",
      "3444\n",
      "3453\n",
      "3457\n",
      "3478\n",
      "3493\n",
      "3520\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3530\n",
      "3543\n",
      "3552\n",
      "3561\n",
      "3562\n",
      "3567\n",
      "3574\n",
      "3580\n",
      "3590\n",
      "3593\n",
      "3596\n",
      "3597\n",
      "3626\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3635\n",
      "3643\n",
      "3652\n",
      "3655\n",
      "3660\n",
      "3675\n",
      "3686\n",
      "3694\n",
      "3695\n",
      "3704\n",
      "3714\n",
      "3724\n",
      "3725\n",
      "3727\n",
      "3728\n",
      "3734\n",
      "3736\n",
      "3739\n",
      "3744\n",
      "3746\n",
      "3748\n",
      "3770\n",
      "3772\n",
      "3776\n",
      "3778\n",
      "3793\n",
      "3794\n",
      "3804\n",
      "3805\n",
      "3817\n",
      "3824\n",
      "3828\n",
      "3833\n",
      "3834\n",
      "3837\n",
      "3838\n",
      "3840\n",
      "3857\n",
      "3858\n",
      "3860\n",
      "3889\n",
      "3903\n",
      "3907\n",
      "3908\n",
      "3919\n",
      "3922\n",
      "3926\n",
      "3952\n",
      "3960\n",
      "3974\n",
      "3981\n",
      "3984\n",
      "3986\n",
      "4005\n",
      "4012\n",
      "4044\n",
      "4048\n",
      "4057\n",
      "4060\n",
      "4067\n",
      "4077\n",
      "4090\n",
      "4093\n",
      "4105\n",
      "4111\n",
      "4116\n",
      "4118\n",
      "4122\n",
      "4130\n",
      "4133\n",
      "4145\n",
      "4147\n",
      "4185\n",
      "4192\n",
      "4200\n",
      "4203\n",
      "4214\n",
      "4219\n",
      "4240\n",
      "4244\n",
      "4245\n",
      "4251\n",
      "4252\n",
      "4269\n",
      "4271\n",
      "4282\n",
      "4300\n",
      "4306\n",
      "4309\n",
      "4314\n",
      "4323\n",
      "4325\n",
      "4335\n",
      "4336\n",
      "4341\n",
      "4342\n",
      "4344\n",
      "4371\n",
      "4377\n",
      "4380\n",
      "4383\n",
      "4384\n",
      "4415\n",
      "4425\n",
      "4427\n",
      "4437\n",
      "4443\n",
      "4449\n",
      "4473\n",
      "4483\n",
      "4488\n",
      "4492\n",
      "4494\n",
      "4503\n",
      "4504\n",
      "4509\n",
      "4511\n",
      "4512\n",
      "4532\n",
      "4534\n",
      "4535\n",
      "4539\n",
      "4541\n",
      "4543\n",
      "4544\n",
      "4560\n",
      "4561\n",
      "4562\n",
      "4566\n",
      "4573\n",
      "4587\n",
      "4588\n",
      "4601\n",
      "4602\n",
      "4605\n",
      "4614\n",
      "4617\n",
      "4620\n",
      "4625\n",
      "4629\n",
      "4635\n",
      "4643\n",
      "4646\n",
      "4659\n",
      "4662\n",
      "4667\n",
      "4672\n",
      "4691\n",
      "4702\n",
      "4715\n",
      "4716\n",
      "4722\n",
      "4723\n",
      "4727\n",
      "4732\n",
      "4739\n",
      "4755\n",
      "4765\n",
      "4766\n",
      "4783\n",
      "4794\n",
      "4803\n",
      "4812\n",
      "4817\n",
      "4819\n",
      "4825\n",
      "4830\n",
      "4833\n",
      "4849\n",
      "4850\n",
      "4853\n",
      "4868\n",
      "4874\n",
      "4883\n",
      "4886\n",
      "4889\n",
      "4892\n",
      "4893\n",
      "4898\n",
      "4906\n",
      "4916\n",
      "4922\n",
      "4935\n",
      "4945\n",
      "4955\n",
      "4978\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "5033\n",
      "5036\n",
      "5073\n",
      "5087\n",
      "5095\n",
      "5115\n",
      "5134\n",
      "5149\n",
      "5151\n",
      "5152\n",
      "5177\n",
      "5180\n",
      "5182\n",
      "5193\n",
      "5214\n",
      "5215\n",
      "5217\n",
      "5239\n",
      "5251\n",
      "5255\n",
      "5262\n",
      "5264\n",
      "5265\n",
      "5271\n",
      "5272\n",
      "5284\n",
      "5296\n",
      "5298\n",
      "5300\n",
      "5303\n",
      "5304\n",
      "5313\n",
      "5321\n",
      "5349\n",
      "5357\n",
      "5369\n",
      "5381\n",
      "5404\n",
      "5411\n",
      "5430\n",
      "5434\n",
      "5458\n",
      "5459\n",
      "5464\n",
      "5476\n",
      "5479\n",
      "5485\n",
      "5490\n",
      "5491\n",
      "5495\n",
      "5496\n",
      "5507\n",
      "5510\n",
      "5515\n",
      "5521\n",
      "5523\n",
      "5529\n",
      "5544\n",
      "5560\n",
      "5565\n",
      "5567\n",
      "5593\n",
      "5597\n",
      "5598\n",
      "5607\n",
      "5619\n",
      "5622\n",
      "5627\n",
      "5635\n",
      "5637\n",
      "5641\n",
      "5644\n",
      "5675\n",
      "5676\n",
      "5714\n",
      "5751\n",
      "5781\n",
      "5786\n",
      "5792\n",
      "5799\n",
      "5806\n",
      "5807\n",
      "5853\n",
      "5854\n",
      "5871\n",
      "5883\n",
      "5892\n",
      "5893\n",
      "5911\n",
      "5915\n",
      "5924\n",
      "5925\n",
      "5928\n",
      "5932\n",
      "5934\n",
      "5944\n",
      "5959\n",
      "5967\n",
      "5977\n",
      "5986\n",
      "6006\n",
      "6020\n",
      "6023\n",
      "6053\n",
      "6069\n",
      "6075\n",
      "6081\n",
      "6090\n",
      "6093\n",
      "6111\n",
      "6116\n",
      "6129\n",
      "6155\n",
      "6163\n",
      "6169\n",
      "6177\n",
      "6186\n",
      "6199\n",
      "6200\n",
      "6210\n",
      "6217\n",
      "6222\n",
      "6223\n",
      "6242\n",
      "6244\n",
      "6246\n",
      "6253\n",
      "6255\n",
      "6261\n",
      "6273\n",
      "6274\n",
      "6288\n",
      "6291\n",
      "6301\n",
      "6306\n",
      "6308\n",
      "6321\n",
      "6324\n",
      "6333\n",
      "6334\n",
      "6341\n",
      "6356\n",
      "6363\n",
      "6366\n",
      "6372\n",
      "6391\n",
      "6393\n",
      "6406\n",
      "6430\n",
      "6435\n",
      "6447\n",
      "6451\n",
      "6453\n",
      "6476\n",
      "6482\n",
      "6495\n",
      "6509\n",
      "6511\n",
      "6529\n",
      "6544\n",
      "6555\n",
      "6557\n",
      "6580\n",
      "6595\n",
      "6601\n",
      "6603\n",
      "6613\n",
      "6614\n",
      "6626\n",
      "6632\n",
      "6633\n",
      "6657\n",
      "6658\n",
      "6676\n",
      "6689\n",
      "6693\n",
      "6694\n",
      "6710\n",
      "6731\n",
      "6732\n",
      "6737\n",
      "6739\n",
      "6756\n",
      "6766\n",
      "6767\n",
      "6768\n",
      "6778\n",
      "6782\n",
      "6787\n",
      "6789\n",
      "6791\n",
      "6814\n",
      "6823\n",
      "6824\n",
      "6828\n",
      "6832\n",
      "6838\n",
      "6848\n",
      "6862\n",
      "6868\n",
      "6870\n",
      "6872\n",
      "6874\n",
      "6878\n",
      "6881\n",
      "6886\n",
      "6889\n",
      "6893\n",
      "6901\n",
      "6909\n",
      "6910\n",
      "6913\n",
      "6914\n",
      "6919\n",
      "6920\n",
      "6922\n",
      "6923\n",
      "6939\n",
      "6952\n",
      "6957\n",
      "6960\n",
      "6980\n",
      "6984\n",
      "6988\n",
      "6991\n",
      "7003\n",
      "7006\n",
      "7010\n",
      "7018\n",
      "7038\n",
      "7040\n",
      "7049\n",
      "7057\n",
      "7081\n",
      "7085\n",
      "7087\n",
      "7088\n",
      "7095\n",
      "7096\n",
      "7097\n",
      "7113\n",
      "7122\n",
      "7143\n",
      "7147\n",
      "7187\n",
      "7202\n",
      "7222\n",
      "7223\n",
      "7229\n",
      "7241\n",
      "7258\n",
      "7266\n",
      "7269\n",
      "7273\n",
      "7276\n",
      "7279\n",
      "7285\n",
      "7292\n",
      "7304\n",
      "7328\n",
      "7329\n",
      "7330\n",
      "7345\n",
      "7378\n",
      "7391\n",
      "7406\n",
      "7418\n",
      "7421\n",
      "7452\n",
      "7459\n",
      "7460\n",
      "7461\n",
      "7464\n",
      "7467\n",
      "7470\n",
      "7473\n",
      "7490\n",
      "7504\n",
      "7520\n",
      "7525\n",
      "7546\n",
      "7553\n",
      "7554\n",
      "7571\n",
      "7575\n",
      "7578\n",
      "7579\n",
      "7597\n",
      "7603\n",
      "7607\n",
      "7639\n",
      "7651\n",
      "7658\n",
      "7667\n",
      "7671\n",
      "7683\n",
      "7695\n",
      "7701\n",
      "7702\n",
      "7712\n",
      "7714\n",
      "7725\n",
      "7729\n",
      "7737\n",
      "7758\n",
      "7790\n",
      "7793\n",
      "7810\n",
      "7822\n",
      "7827\n",
      "7833\n",
      "7835\n",
      "7840\n",
      "7848\n",
      "7850\n",
      "7853\n",
      "7855\n",
      "7859\n",
      "7876\n",
      "7884\n",
      "7886\n",
      "7907\n",
      "7917\n",
      "7928\n",
      "7942\n",
      "7943\n",
      "7949\n",
      "7955\n",
      "7957\n",
      "7974\n",
      "7976\n",
      "7988\n",
      "7990\n",
      "7995\n",
      "7997\n",
      "8006\n",
      "8022\n",
      "8030\n",
      "8041\n",
      "8051\n",
      "8052\n",
      "8066\n",
      "8068\n",
      "8119\n",
      "8123\n",
      "8125\n",
      "8126\n",
      "8130\n",
      "8141\n",
      "8144\n",
      "8163\n",
      "8167\n",
      "8169\n",
      "8198\n",
      "8200\n",
      "8208\n",
      "8210\n",
      "8220\n",
      "8236\n",
      "8238\n",
      "8243\n",
      "8250\n",
      "8254\n",
      "8263\n",
      "8273\n",
      "8278\n",
      "8286\n",
      "8292\n",
      "8308\n",
      "8309\n",
      "8316\n",
      "8332\n",
      "8344\n",
      "8345\n",
      "8348\n",
      "8352\n",
      "8354\n",
      "8355\n",
      "8361\n",
      "8369\n",
      "8374\n",
      "8377\n",
      "8389\n",
      "8392\n",
      "8398\n",
      "8424\n",
      "8427\n",
      "8428\n",
      "8445\n",
      "8460\n",
      "8483\n",
      "8493\n",
      "8495\n",
      "8499\n",
      "8515\n",
      "8519\n",
      "8524\n",
      "8528\n",
      "8530\n",
      "8531\n",
      "8544\n",
      "8549\n",
      "8574\n",
      "8583\n",
      "8585\n",
      "8595\n",
      "8599\n",
      "8611\n",
      "8622\n",
      "8624\n",
      "8648\n",
      "8652\n",
      "8663\n",
      "8679\n",
      "8684\n",
      "8693\n",
      "8704\n",
      "8710\n",
      "8711\n",
      "8720\n",
      "8722\n",
      "8727\n",
      "8730\n",
      "8745\n",
      "8750\n",
      "8770\n",
      "8785\n",
      "8821\n",
      "8838\n",
      "8848\n",
      "8859\n",
      "8861\n",
      "8865\n",
      "8885\n",
      "8895\n",
      "8910\n",
      "8917\n",
      "8918\n",
      "8923\n",
      "8925\n",
      "8932\n",
      "8945\n",
      "8960\n",
      "8972\n",
      "8978\n",
      "8991\n",
      "9029\n",
      "9031\n",
      "9052\n",
      "9073\n",
      "9076\n",
      "9087\n",
      "9095\n",
      "9117\n",
      "9125\n",
      "9127\n",
      "9132\n",
      "9145\n",
      "9153\n",
      "9164\n",
      "9165\n",
      "9172\n",
      "9173\n",
      "9216\n",
      "9222\n",
      "9237\n",
      "9239\n",
      "9243\n",
      "9253\n",
      "9255\n",
      "9270\n",
      "9297\n",
      "9309\n",
      "9363\n",
      "9366\n",
      "9382\n",
      "9388\n",
      "9389\n",
      "9398\n",
      "9411\n",
      "9421\n",
      "9423\n",
      "9434\n",
      "9448\n",
      "9462\n",
      "9475\n",
      "9476\n",
      "9483\n",
      "9484\n",
      "9494\n",
      "9506\n",
      "9509\n",
      "9529\n",
      "9539\n",
      "9545\n",
      "9547\n",
      "9555\n",
      "9573\n",
      "9602\n",
      "9613\n",
      "9623\n",
      "9624\n",
      "9628\n",
      "9638\n",
      "9659\n",
      "9669\n",
      "9685\n",
      "9687\n",
      "9701\n",
      "9713\n",
      "9731\n",
      "9740\n",
      "9747\n",
      "9753\n",
      "9757\n",
      "9761\n",
      "9769\n",
      "9778\n",
      "9792\n",
      "9802\n",
      "9805\n",
      "9808\n",
      "9818\n",
      "9827\n",
      "9832\n",
      "9834\n",
      "9835\n",
      "9836\n",
      "9837\n",
      "9851\n",
      "9858\n",
      "9875\n",
      "9876\n",
      "9880\n",
      "9897\n",
      "9915\n",
      "9920\n",
      "9928\n",
      "9935\n",
      "9949\n",
      "9952\n",
      "9956\n",
      "9971\n",
      "9993\n",
      "9995\n",
      "10001\n",
      "10012\n",
      "10015\n",
      "10016\n",
      "10024\n",
      "10026\n",
      "10028\n",
      "10029\n",
      "10030\n",
      "10045\n",
      "10054\n",
      "10065\n",
      "10067\n",
      "10073\n",
      "10083\n",
      "10112\n",
      "10135\n",
      "10137\n",
      "10139\n",
      "10156\n",
      "10177\n",
      "10187\n",
      "10199\n",
      "10204\n",
      "10209\n",
      "10210\n",
      "10230\n",
      "10268\n",
      "10289\n",
      "10292\n",
      "10295\n",
      "10297\n",
      "10300\n",
      "10309\n",
      "10324\n",
      "10328\n",
      "10348\n",
      "10355\n",
      "10356\n",
      "10359\n",
      "10362\n",
      "10363\n",
      "10367\n",
      "10381\n",
      "10388\n",
      "10389\n",
      "10390\n",
      "10398\n",
      "10401\n",
      "10407\n",
      "10410\n",
      "10425\n",
      "10429\n",
      "10436\n",
      "10446\n",
      "10454\n",
      "10477\n",
      "10486\n",
      "10502\n",
      "10512\n",
      "10524\n",
      "10546\n",
      "10561\n",
      "10562\n",
      "10564\n",
      "10572\n",
      "10573\n",
      "10584\n",
      "10586\n",
      "10624\n",
      "10628\n",
      "10667\n",
      "10695\n",
      "10698\n",
      "10699\n",
      "10708\n",
      "10713\n",
      "10714\n",
      "10717\n",
      "10739\n",
      "10756\n",
      "10782\n",
      "10783\n",
      "10786\n",
      "10788\n",
      "10805\n",
      "10812\n",
      "10816\n",
      "10820\n",
      "10823\n",
      "10835\n",
      "10836\n",
      "10848\n",
      "10862\n",
      "10866\n",
      "10904\n",
      "10905\n",
      "10908\n",
      "10911\n",
      "10912\n",
      "10928\n",
      "10929\n",
      "10931\n",
      "10949\n",
      "10967\n",
      "10977\n",
      "10979\n",
      "10989\n",
      "10992\n",
      "10995\n",
      "11002\n",
      "11016\n",
      "11017\n",
      "11029\n",
      "11036\n",
      "11051\n",
      "11061\n",
      "11069\n",
      "11070\n",
      "11075\n",
      "11089\n",
      "11103\n",
      "11124\n",
      "11127\n",
      "11128\n",
      "11132\n",
      "11142\n",
      "11152\n",
      "11161\n",
      "11170\n",
      "11173\n",
      "11184\n",
      "11191\n",
      "11194\n",
      "11201\n",
      "11202\n",
      "11212\n",
      "11214\n",
      "11224\n",
      "11226\n",
      "11232\n",
      "11237\n",
      "11238\n",
      "11239\n",
      "11269\n",
      "11284\n",
      "11291\n",
      "11361\n",
      "11362\n",
      "11366\n",
      "11375\n",
      "11383\n",
      "11391\n",
      "11407\n",
      "11409\n",
      "11431\n",
      "11435\n",
      "11438\n",
      "11458\n",
      "11459\n",
      "11467\n",
      "11471\n",
      "11479\n",
      "11503\n",
      "11507\n",
      "11508\n",
      "11512\n",
      "11518\n",
      "11520\n",
      "11529\n",
      "11534\n",
      "11540\n",
      "11543\n",
      "11546\n",
      "11564\n",
      "11589\n",
      "11592\n",
      "11593\n",
      "11610\n",
      "11611\n",
      "11616\n",
      "11617\n",
      "11619\n",
      "11625\n",
      "11632\n",
      "11643\n",
      "11647\n",
      "11650\n",
      "11666\n",
      "11675\n",
      "11691\n",
      "11709\n",
      "11711\n",
      "11730\n",
      "11741\n",
      "11746\n",
      "11752\n",
      "11756\n",
      "11757\n",
      "11790\n",
      "11792\n",
      "11808\n",
      "11812\n",
      "11817\n",
      "11850\n",
      "11866\n",
      "11870\n",
      "11872\n",
      "11874\n",
      "11904\n",
      "11907\n",
      "11911\n",
      "11932\n",
      "11956\n",
      "11964\n",
      "11970\n",
      "11980\n",
      "11985\n",
      "12000\n",
      "12006\n",
      "12008\n",
      "12014\n",
      "12016\n",
      "12019\n",
      "12027\n",
      "12032\n",
      "12039\n",
      "12040\n",
      "12043\n",
      "12047\n",
      "12054\n",
      "12066\n",
      "12082\n",
      "12089\n",
      "12092\n",
      "12094\n",
      "12103\n",
      "12104\n",
      "12105\n",
      "12111\n",
      "12115\n",
      "12117\n",
      "12120\n",
      "12124\n",
      "12133\n",
      "12141\n",
      "12155\n",
      "12163\n",
      "12165\n",
      "12168\n",
      "12179\n",
      "12181\n",
      "12182\n",
      "12192\n",
      "12205\n",
      "12214\n",
      "12226\n",
      "12227\n",
      "12232\n",
      "12234\n",
      "12246\n",
      "12250\n",
      "12253\n",
      "12264\n",
      "12276\n",
      "12283\n",
      "12292\n",
      "12302\n",
      "12308\n",
      "12309\n",
      "12316\n",
      "12317\n",
      "12329\n",
      "12339\n",
      "12342\n",
      "12345\n",
      "12351\n",
      "12354\n",
      "12363\n",
      "12369\n",
      "12375\n",
      "12377\n",
      "12396\n",
      "12409\n",
      "12412\n",
      "12416\n",
      "12434\n",
      "12448\n",
      "12463\n",
      "12474\n",
      "12479\n",
      "12501\n",
      "12502\n",
      "12519\n",
      "12520\n",
      "12522\n",
      "12525\n",
      "12538\n",
      "12545\n",
      "12546\n",
      "12549\n",
      "12589\n",
      "12594\n",
      "12608\n",
      "12609\n",
      "12613\n",
      "12614\n",
      "12616\n",
      "12627\n",
      "Loss = 0.514334\n",
      "Acc = 88.456057 percent\n"
     ]
    }
   ],
   "source": [
    "loss, acc = evaluate(net, test_X, test_Y)\n",
    "\n",
    "print(\"Loss = %.6f\" % (loss))\n",
    "print(\"Acc = %.6f percent\" % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = evaluate(net, X, Y)\n",
    "\n",
    "print(\"Loss = %.6f\" % (loss))\n",
    "print(\"Acc = %.6f percent\" % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist)\n",
    "plt.plot(hist)\n",
    "plt.ylabel('Loss of training set')\n",
    "plt.xlabel('Number of epoches')\n",
    "plt.show()\n",
    "\n",
    "start = time.time()\n",
    "loss, acc = evaluate(net, test_X, test_Y)\n",
    "now = time.time()\n",
    "print(\"Test set loss = %.6f\" % (loss))\n",
    "print(\"Test set acc = %.6f percent\" % (acc*100))\n",
    "print(\"Time per image: %.6f seconds: \" % ((now-start)/test_X.shape[0]))\n",
    "\n",
    "start = time.time()\n",
    "loss, acc = evaluate(net, X, Y)\n",
    "now = time.time()\n",
    "print(\"Train set loss = %.6f\" % (loss))\n",
    "print(\"Train set acc = %.6f percent\" % (acc*100))\n",
    "print(\"Time per image: %.6f seconds: \" % ((now-start)/X.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist)\n",
    "plt.plot(hist)\n",
    "plt.ylabel('Loss of training set')\n",
    "plt.xlabel('Number of epoches')\n",
    "plt.show()\n",
    "\n",
    "start = time.time()\n",
    "loss, acc = evaluate(net, test_X, test_Y)\n",
    "now = time.time()\n",
    "print(\"Test set loss = %.6f\" % (loss))\n",
    "print(\"Test set acc = %.6f percent\" % (acc*100))\n",
    "print(\"Time per image: %.6f seconds \" % ((now-start)/test_X.shape[0]))\n",
    "\n",
    "start = time.time()\n",
    "loss, acc = evaluate(net, X, Y)\n",
    "now = time.time()\n",
    "print(\"Train set loss = %.6f\" % (loss))\n",
    "print(\"Train set acc = %.6f percent\" % (acc*100))\n",
    "print(\"Time per image: %.6f seconds \" % ((now-start)/X.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
