{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from load_dataset import get_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39209\n",
      "(39209, 4096)\n",
      "(39209,)\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_dataset()\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backprop(W1, b1, W2, b2, X, y=None, reg=0):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    dW2 = np.zeros_like(W2)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "    # regularization!                                                           #\n",
    "    #############################################################################\n",
    "    num_train = X.shape[0]\n",
    "    num_hidden = W1.shape[1]\n",
    "    num_classes = W2.shape[1]\n",
    "\n",
    "    # Forward.\n",
    "    H = X.dot(W1) + b1 # M x Nh\n",
    "    H_relu = H.copy()\n",
    "    H_relu[H_relu < 0] = 0\n",
    "    \n",
    "    scores = H_relu.dot(W2) + b2\n",
    "    scores -= np.max(scores, axis=1)[:,np.newaxis]\n",
    "    scores = np.exp(scores) / np.sum(np.exp(scores), axis=1)[:,np.newaxis]\n",
    "    \n",
    "    loss = np.sum(-np.log(scores[np.arange(num_train),y]))\n",
    "    loss /= num_train\n",
    "    loss += reg * (np.sum(W1 * W1) + np.sum(W2 * W2))\n",
    "    \n",
    "    if y is None:\n",
    "        return loss\n",
    "    \n",
    "    # Backward.\n",
    "    grads = {}\n",
    "    dscores = scores.copy()\n",
    "    dscores[np.arange(num_train),y] -= 1\n",
    "    dscores /= num_train\n",
    "\n",
    "    dW2 = H_relu.T.dot(dscores) + 2 * reg * W2\n",
    "    db2 = np.sum(dscores, axis=0)\n",
    "    \n",
    "    dH_relu = dscores.dot(W2.T)\n",
    "    dH = (H >= 0) * dH_relu\n",
    "    \n",
    "    dW1 = X.T.dot(dH) + 2 * reg * W1\n",
    "    db1 = np.sum(dH, axis=0)\n",
    "\n",
    "    grads[\"W2\"] = dW2\n",
    "    grads[\"b2\"] = db2\n",
    "    grads[\"W1\"] = dW1\n",
    "    grads[\"b1\"] = db1\n",
    "    \n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.rand(4096, 64)*0.001\n",
    "b1 = np.random.rand(64)*0.001\n",
    "W2 = np.random.rand(64, 43)*0.001\n",
    "b2 = np.random.rand(43)*0.001\n",
    "loss, grads = forward_and_backprop(W1, b1, W2, b2, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58940648465\n",
      "{'b1': array([  8.05426341e-05,  -4.18433140e-05,   1.92464754e-05,\n",
      "        -7.03338717e-07,   2.38945794e-05,   2.30595226e-05,\n",
      "         3.71371723e-05,   1.10810525e-04,   1.39167067e-04,\n",
      "         1.80137385e-04,   1.47810418e-04,   1.13109601e-04,\n",
      "        -5.18818787e-05,   1.65533231e-05,   1.12067036e-05,\n",
      "         5.28565018e-06,  -8.31273936e-05,   1.12586564e-04,\n",
      "         1.27886035e-04,  -5.19944394e-05,   2.26758140e-05,\n",
      "         1.34584281e-04,   3.09668588e-05,  -9.61174793e-07,\n",
      "         6.60720922e-05,   9.80169765e-05,  -5.57116356e-05,\n",
      "        -1.43088306e-05,   1.22362691e-04,   2.93043535e-05,\n",
      "        -1.20224948e-04,  -5.18436157e-05,   8.48347803e-06,\n",
      "         9.83921248e-05,   6.28317287e-05,   2.17668177e-05,\n",
      "        -7.70435600e-05,  -3.63415425e-05,   3.90055239e-05,\n",
      "         7.53093774e-05,   8.82911448e-06,   2.45227203e-06,\n",
      "         2.48733005e-05,   1.33642405e-05,   1.11840243e-04,\n",
      "         1.00492736e-04,   1.26297589e-04,  -3.74029490e-05,\n",
      "        -2.34406764e-05,   5.18284437e-05,   1.29138137e-04,\n",
      "        -1.19874661e-04,  -1.57937044e-05,   6.37235667e-05,\n",
      "         8.92383464e-05,  -4.44419674e-05,   9.42780665e-05,\n",
      "         8.58225051e-05,   1.27114234e-05,   1.29071186e-04,\n",
      "        -1.76992741e-05,   1.27005029e-04,   2.91734646e-05,\n",
      "         2.79324392e-05]), 'W2': array([[  0.14139171, -27.55219806, -29.92951406, ...,  -2.54304777,\n",
      "         -0.21292855,   4.64127547],\n",
      "       [  0.1451327 , -27.79946255, -30.23566006, ...,  -2.55746582,\n",
      "         -0.22603966,   4.68267891],\n",
      "       [  0.14811949, -27.70972834, -30.12587571, ...,  -2.55818977,\n",
      "         -0.22099953,   4.6730544 ],\n",
      "       ..., \n",
      "       [  0.13847789, -27.80009789, -30.2134879 , ...,  -2.55434814,\n",
      "         -0.23490488,   4.67129721],\n",
      "       [  0.13802378, -27.63731176, -30.05138796, ...,  -2.55422478,\n",
      "         -0.22636746,   4.65154333],\n",
      "       [  0.14640568, -27.75471194, -30.20945307, ...,  -2.55190573,\n",
      "         -0.2236768 ,   4.6774512 ]]), 'W1': array([[ 0.00601214, -0.00394661,  0.00299   , ...,  0.01252874,\n",
      "         0.00295483,  0.00238322],\n",
      "       [ 0.00615685, -0.00378021,  0.00269349, ...,  0.01218263,\n",
      "         0.00291991,  0.00225524],\n",
      "       [ 0.00657351, -0.00373912,  0.00232529, ...,  0.01181259,\n",
      "         0.00280067,  0.00235823],\n",
      "       ..., \n",
      "       [ 0.00393773, -0.0035208 ,  0.0028032 , ...,  0.01042097,\n",
      "         0.00185761,  0.00178589],\n",
      "       [ 0.00385589, -0.00336604,  0.00253621, ...,  0.01010605,\n",
      "         0.0018615 ,  0.00170476],\n",
      "       [ 0.00404623, -0.0032575 ,  0.0022259 , ...,  0.00979671,\n",
      "         0.00177668,  0.00181313]]), 'b2': array([ 0.00646049, -0.03397983, -0.04987611, -0.00536163, -0.04450976,\n",
      "       -0.04149115,  0.0141165 , -0.01935258, -0.03059817, -0.02209737,\n",
      "       -0.0373864 , -0.0056333 , -0.04547912, -0.04622128, -0.01133906,\n",
      "       -0.00914123,  0.18732868, -0.01052798,  0.01815581,  0.01450437,\n",
      "        0.01494465,  0.02172943, -0.00385154,  0.00569606,  0.01039801,\n",
      "        0.06984621, -0.00422908,  0.03366507,  0.00520533,  0.0409386 ,\n",
      "        0.02509414, -0.01179117,  0.0006477 , -0.00470474, -0.00293898,\n",
      "       -0.01803932,  0.00580738,  0.00811861, -0.04782862,  0.00954751,\n",
      "       -0.00020897,  0.00371677,  0.01066606])}\n"
     ]
    }
   ],
   "source": [
    "def train_data(W1, b1, W2, b2, data_x, data_y, lr=1e-6, batch_size=128, num_epoch=10):\n",
    "    num_train = data_x.shape[0]\n",
    "    iterate_per_epoch = int(num_train / batch_size) + 1\n",
    "    for i in range(num_epoch):\n",
    "        for j in range(iterate_per_epoch):\n",
    "            indices = np.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
